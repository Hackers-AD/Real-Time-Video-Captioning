{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454d03bb",
   "metadata": {},
   "source": [
    "<h1>Real-Time Video Captioning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198345e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65719cf1",
   "metadata": {},
   "source": [
    "<h3>Importing Libraries and Dependecies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc8ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, Concatenate, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb2328",
   "metadata": {},
   "source": [
    "<h3>Data Extraction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d664a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"captions.txt\", nrows=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2adf0b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>A black dog and a spotted dog are fighting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "5  1001773457_577c3a7d70.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  \n",
       "5         A black dog and a spotted dog are fighting  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1995fe9",
   "metadata": {},
   "source": [
    "<h3>Data Exploratory Analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28eff48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd1a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image', 'caption'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "632ba231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image      object\n",
       "caption    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ee7f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   image    2500 non-null   object\n",
      " 1   caption  2500 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b706bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            image  \\\n",
       "count                        2500   \n",
       "unique                        500   \n",
       "top     1000268201_693b08cb0e.jpg   \n",
       "freq                            5   \n",
       "\n",
       "                                                  caption  \n",
       "count                                                2500  \n",
       "unique                                               2500  \n",
       "top     A child in a pink dress is climbing up a set o...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c1c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ad8885",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing Steps</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c01c75",
   "metadata": {},
   "source": [
    "<h4>1. Validation and Cleansing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97683457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image      0\n",
       "caption    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3787d9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ee2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f32d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee588e",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5488bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_model = VGG16()\n",
    "img_model = Model(inputs=img_model.inputs, outputs=img_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf9d8c2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134260544 (512.16 MB)\n",
      "Trainable params: 134260544 (512.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4883112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_model.output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract features from image\n",
    "# img_features = {}\n",
    "# directory = \"../dataset/images\"\n",
    "\n",
    "# for name_img in tqdm(df['image']):\n",
    "#     path_img = directory + '/' + name_img\n",
    "#     img = load_img(path_img, target_size=(224, 224))\n",
    "#     img = img_to_array(img)\n",
    "#     img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "#     img = preprocess_input(img)\n",
    "#     feat = img_model.predict(img, verbose=0)\n",
    "#     id_img = name_img.split('.')[0]\n",
    "#     img_features[id_img] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d81254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../dataset/features.pkl\", 'wb') as file:\n",
    "#     pickle.dump(img_features, file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a143c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/img_features.npy\", 'rb') as file:\n",
    "    img_features = np.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65dadbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40445, 4096)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b6dfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mappings(df):\n",
    "    mappings = {}\n",
    "    \n",
    "    for idx, img in enumerate(df['image']):\n",
    "        img_name = img.split('.')[0]\n",
    "        mappings[img_name] = mappings.get(img_name, []) + [df.iloc[idx]['caption']] \n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d6ad821",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = create_mappings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1b945dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_captions(mappings):\n",
    "    for cap_key, cap_list in mappings.items():\n",
    "        for i in range(len(cap_list)):\n",
    "            text = cap_list[i]\n",
    "            text = text.lower()\n",
    "            text = text.replace('[^A-Za-z]', '')\n",
    "            text = text.replace('\\s+', ' ')\n",
    "            text = 'startseq ' + \" \".join([word for word in text.split() if len(word)>1]) + ' endseq'\n",
    "            cap_list[i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4245127",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_captions(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65ae30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for idx in mappings:\n",
    "    for cap in mappings[idx]:\n",
    "        all_captions.append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a1e65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab032188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f0ab3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max([len(cap_text.split()) for cap_text in all_captions])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "471de406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Provide the path to your downloaded GloVe file\n",
    "glove_file_path = '../dataset/embed/glove.6B.100d.txt'  # Change the path and dimensionality accordingly\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50080be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix for your vocabulary\n",
    "embedding_dim = 100  # Change the dimensionality based on your GloVe model\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "958c22fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2174, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56073160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40445, 4096)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46f0f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(img_ids, mappings, img_features, tokenizer, maxlen, vocab_size, batch_size):\n",
    "    while True:\n",
    "        IN1, IN2, OUT = [], [], []\n",
    "        \n",
    "        for id_img in img_ids:\n",
    "            cap_list = mappings[id_img]\n",
    "            \n",
    "            for caption in cap_list:\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=maxlen)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    IN1.append(img_features[id_img][0])\n",
    "                    IN2.append(in_seq)\n",
    "                    OUT.append(out_seq)\n",
    "                    \n",
    "                    if len(IN1) == batch_size:\n",
    "                        yield [np.array(IN1), np.array(IN2)], np.array(OUT)\n",
    "                        IN1, IN2, OUT = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cbffde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mappings.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "741efe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 29)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 4096)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 29, 100)              217400    ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 4096)                 0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 29, 100)              0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  1048832   ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 256)                  365568    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 256)                  0         ['dense[0][0]',               \n",
      "                                                                     'lstm[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 256)                  65792     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 2174)                 558718    ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2256310 (8.61 MB)\n",
      "Trainable params: 2038910 (7.78 MB)\n",
      "Non-trainable params: 217400 (849.22 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_input = Input(shape=img_model.layers[-2].output.shape[1],)\n",
    "dropout_image = Dropout(rate=0.45) (image_input)\n",
    "dense_image = Dense(256, activation=\"relu\")(dropout_image)\n",
    "\n",
    "caption_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length,\n",
    "                           weights=[embedding_matrix], trainable=False)(caption_input)\n",
    "dropout_caption = Dropout(rate=0.45) (embedding_layer)\n",
    "lstm = LSTM(256, return_sequences=False)(dropout_caption)\n",
    "\n",
    "\n",
    "merged_layer = add([dense_image, lstm])\n",
    "decoder = Dense(256, activation='relu')(merged_layer)\n",
    "output = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "# Create the caption generation model\n",
    "caption_model = Model(inputs=[image_input, caption_input], outputs=output)\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9a427c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m generator \u001b[38;5;241m=\u001b[39m gen_data(train, mappings, img_features, tokenizer, max_length, vocab_size, batch_size)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# fit for one epoch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m caption_model\u001b[38;5;241m.\u001b[39mfit(generator, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, steps_per_epoch\u001b[38;5;241m=\u001b[39msteps, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[40], line 17\u001b[0m, in \u001b[0;36mgen_data\u001b[1;34m(img_ids, mappings, img_features, tokenizer, maxlen, vocab_size, batch_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m in_seq \u001b[38;5;241m=\u001b[39m pad_sequences([in_seq], maxlen\u001b[38;5;241m=\u001b[39mmaxlen)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m out_seq \u001b[38;5;241m=\u001b[39m to_categorical([out_seq], num_classes\u001b[38;5;241m=\u001b[39mvocab_size)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 17\u001b[0m IN1\u001b[38;5;241m.\u001b[39mappend(img_features[id_img][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     18\u001b[0m IN2\u001b[38;5;241m.\u001b[39mappend(in_seq)\n\u001b[0;32m     19\u001b[0m OUT\u001b[38;5;241m.\u001b[39mappend(out_seq)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = gen_data(train, mappings, img_features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    caption_model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e929865",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./videocaptioningUI/caption_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(caption_model, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bf975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b77552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "      \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a505454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def generate_caption(image_name):\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = f\"../dataset/images/{image_name}\"\n",
    "    image = Image.open(img_path)\n",
    "    captions = mappings[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(caption_model, img_features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c090a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f0683",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"1463732807_0cdf4f22c7.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02502fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../dataset/sample_img/sample3.jpg'\n",
    "# load image\n",
    "image = load_img(image_path, target_size=(224, 224))\n",
    "# convert image pixels to numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# preprocess image for vgg\n",
    "image = preprocess_input(image)\n",
    "# extract features\n",
    "feature = img_model.predict(image, verbose=0)\n",
    "# predict from the trained model\n",
    "predict_caption(caption_model, feature, tokenizer, max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
